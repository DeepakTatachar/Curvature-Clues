{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import logging\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from utils.load_dataset import load_dataset\n",
    "\n",
    "with open(\"./config.json\") as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "logger = logging.getLogger(\"\")\n",
    "dataset_name = 'cifar100'\n",
    "arch = 'resnet18'\n",
    "num_augs = 2\n",
    "\n",
    "dataset = load_dataset(\n",
    "    dataset=dataset_name,\n",
    "    train_batch_size=128,\n",
    "    test_batch_size=128,\n",
    "    val_split=0.0,\n",
    "    augment=False,\n",
    "    shuffle=False,\n",
    "    root_path=config['data_dir'],\n",
    "    random_seed=0,\n",
    "    mean=[0, 0, 0],\n",
    "    std=[1, 1, 1],\n",
    "    logger=logger,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scores_and_masks(seeds, dataset, method, num_augs):\n",
    "    h = 0.001\n",
    "    num_seeds = len(seeds)\n",
    "    aug_scores_file_name = {\n",
    "        'curv_zo' : \"curvature_scores_zo_{}_{}_{}_{}_tid{}.pt\",\n",
    "        'curv' : \"curvature_scores_{}_{}_{}_{}_tid{}.pt\",\n",
    "        'prob' : \"prob_{}_{}_{}_tid{}.pt\",\n",
    "        'loss' : \"losses_{}_{}_{}_tid{}.pt\",\n",
    "        'mentr' : \"m_entropy_{}_{}_{}_tid{}.pt\",\n",
    "        'loss_g' : \"loss_g_{}_{}_{}_tid{}.pt\",\n",
    "    }\n",
    "\n",
    "    score_seeds_v_seed = np.zeros((num_augs, dataset.train_length, num_seeds))\n",
    "    mask_seeds_v_seed = np.zeros((dataset.train_length, num_seeds))\n",
    "    start_seed = seeds[0]\n",
    "    for exp_idx in seeds:\n",
    "        array_idx = exp_idx - start_seed\n",
    "        augs = 1 if method == 'mentr' else num_augs\n",
    "        for aug_idx in range(augs):\n",
    "            if 'curv' in aug_scores_file_name[method]:\n",
    "                file_name = aug_scores_file_name[method].format(dataset.name, arch, exp_idx, h, aug_idx)\n",
    "            else:\n",
    "                file_name = aug_scores_file_name[method].format(dataset.name, arch, exp_idx, aug_idx)\n",
    "\n",
    "            absolute_file_name = os.path.join(config['precomputed_scores_dir'], dataset.name, file_name)\n",
    "            scores = np.load(absolute_file_name)['data']\n",
    "            score_seeds_v_seed[aug_idx, :, array_idx] = scores\n",
    "\n",
    "        if dataset == 'imagenet':\n",
    "            array = np.load(f'/path/imagenet-resnet50/{0.7}/{exp_idx}/aux_arrays.npz')\n",
    "            mask_idxs = array['subsample_idx']\n",
    "            mask_seeds_v_seed[mask_idxs, array_idx] = 1\n",
    "        else:\n",
    "            mask_idxs = np.load(os.path.join(config['subset_idxs_dir'], f\"{exp_idx}.npy\"))\n",
    "            mask_seeds_v_seed[mask_idxs, array_idx] = 1\n",
    "\n",
    "    return score_seeds_v_seed, mask_seeds_v_seed\n",
    "\n",
    "def get_score_dist_params(scores, masks, num_augs):\n",
    "    means_in = []\n",
    "    means_out = []\n",
    "    var_in = []\n",
    "    var_out = []\n",
    "    for aug in range(num_augs):\n",
    "        in_scores = np.where(masks == 1, scores[aug], np.NaN)\n",
    "        out_scores = np.where(masks == 0, scores[aug], np.NaN)\n",
    "        means_in.append(np.nanmean(in_scores, axis=1))\n",
    "        means_out.append(np.nanmean(out_scores, axis=1))\n",
    "        var_in.append(np.nanvar(in_scores, axis=1))\n",
    "        var_out.append(np.nanvar(out_scores, axis=1))\n",
    "\n",
    "    return {\n",
    "        'means_in': np.row_stack(means_in),\n",
    "        'means_out': np.row_stack(means_out),\n",
    "        'var_in': np.row_stack(var_in),\n",
    "        'var_out': np.row_stack(var_out)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = {}\n",
    "masks = {}\n",
    "dist_params = {}\n",
    "train_end_seed = 64\n",
    "train_seeds = list(range(train_end_seed))\n",
    "test_seeds = list(range(train_end_seed, train_end_seed + 3))\n",
    "eps = 1e-22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_err(x):\n",
    "    return np.log(x + eps)\n",
    "\n",
    "def logit_scale(x):\n",
    "    x = x / (1 - x + eps)\n",
    "    return log_err(x)\n",
    "\n",
    "def nll(curv_score, mean, var):\n",
    "    ll = - ( ( (curv_score - mean)**2) / (2 * (var ** 2) ) ) -0.5 * np.log(var ** 2) - 0.5 * np.log(2 * np.pi)\n",
    "    return -ll\n",
    "\n",
    "def likelihood(curv_score, mean, var):\n",
    "    nll = - ( ( (curv_score - mean)**2) / (2 * (var ** 2) ) ) - 0.5 * np.log(var ** 2) - 0.5 * np.log(2 * np.pi)\n",
    "    likelihood_val = np.exp(nll)\n",
    "    return likelihood_val\n",
    "\n",
    "def get_likelihood_ratio(test_scores, dist_params, num_augs, train_scores, train_masks):\n",
    "    in_likelihood = np.zeros((test_scores.shape[1]))\n",
    "    out_likelihood = np.zeros_like(in_likelihood)\n",
    "    for aug in range(num_augs):\n",
    "        test_score_aug = test_scores[aug, :, 0]\n",
    "        in_likelihood += likelihood(\n",
    "            test_score_aug, \n",
    "            dist_params['means_in'][aug], \n",
    "            dist_params['var_in'][aug] + eps)\n",
    "\n",
    "        out_likelihood += likelihood(\n",
    "            test_score_aug, \n",
    "            dist_params['means_out'][aug], \n",
    "            dist_params['var_out'][aug] + eps)\n",
    "\n",
    "    likelihood_ratio = in_likelihood / (out_likelihood + eps)\n",
    "    return likelihood_ratio\n",
    "\n",
    "def get_nll_ratio(test_scores, dist_params, num_augs, train_scores, train_masks):\n",
    "    in_likelihood = np.zeros((test_scores.shape[1]))\n",
    "    out_likelihood = np.zeros_like(in_likelihood)\n",
    "    for aug in range(num_augs):\n",
    "        test_score_aug = test_scores[aug, :, 0]\n",
    "        in_likelihood += nll(\n",
    "            test_score_aug, \n",
    "            dist_params['means_in'][aug], \n",
    "            dist_params['var_in'][aug] + 1e-32)\n",
    "\n",
    "        out_likelihood += nll(\n",
    "            test_score_aug, \n",
    "            dist_params['means_out'][aug], \n",
    "            dist_params['var_in'][aug] + 1e-32)\n",
    "\n",
    "    likelihood_ratio = in_likelihood - out_likelihood\n",
    "    return -likelihood_ratio\n",
    "\n",
    "def get_nll_ratio_c(test_scores, dist_params, num_augs, train_scores, train_masks):\n",
    "    in_likelihood = np.zeros((test_scores.shape[1]))\n",
    "    out_likelihood = np.zeros_like(in_likelihood)\n",
    "    for aug in range(num_augs):\n",
    "        test_score_aug = test_scores[aug, :, 0]\n",
    "        in_likelihood += nll(\n",
    "            test_score_aug, \n",
    "            dist_params['means_in'][aug], \n",
    "            1)\n",
    "\n",
    "        out_likelihood += nll(\n",
    "            test_score_aug, \n",
    "            dist_params['means_out'][aug], \n",
    "            1)\n",
    "\n",
    "    likelihood_ratio = in_likelihood - out_likelihood\n",
    "    return -likelihood_ratio\n",
    "\n",
    "def get_identity(test_scores, dist_params, num_augs, train_scores, train_masks):\n",
    "    return -test_scores[0]\n",
    "\n",
    "def get_mast(test_scores, dist_params, num_augs, train_scores, train_masks):\n",
    "    aug = 0\n",
    "    threshold = (dist_params['means_in'][aug] +  dist_params['means_out'][aug]) / 2 \n",
    "    return -(test_scores[aug, :, 0] - threshold)\n",
    "\n",
    "def get_mast_offline(test_scores, dist_params, num_augs, train_scores, train_masks):\n",
    "    aug = 0\n",
    "    threshold = dist_params['means_out'][aug]\n",
    "    return -(test_scores[aug, :, 0] - threshold)\n",
    "\n",
    "def get_ye_et_el_attack_r(test_scores, dist_params, num_augs, train_scores, train_masks):\n",
    "    aug = 0\n",
    "    alpha = 1e-4\n",
    "    out_scores = np.where(train_masks == 0, train_scores[aug], np.NaN)\n",
    "    threshold = np.nanpercentile(out_scores, alpha * 100, axis=1)\n",
    "    return -(test_scores[aug, :, 0] - threshold)\n",
    "\n",
    "def get_class_based(test_scores, dist_params, num_augs, train_scores, train_masks):\n",
    "    # This is handled during roc calculations\n",
    "    return -test_scores[0, :, 0]\n",
    "\n",
    "def get_loss_count(test_scores, dist_params, num_augs, train_scores, train_masks):\n",
    "    in_scores = np.where(train_masks == 1, train_scores[0], np.NaN)\n",
    "    threshold = np.nanmean(in_scores)\n",
    "    pred = (test_scores[:, :, 0] < threshold).sum(0) / num_augs\n",
    "    return pred\n",
    "\n",
    "methods = [\n",
    "    ('curv_zo', log_err, get_nll_ratio, 'Curv ZO NLL', num_augs, train_seeds), \n",
    "    ('curv_zo', log_err, get_likelihood_ratio, 'Curv ZO LR', num_augs, train_seeds), \n",
    "    ('prob', logit_scale, get_likelihood_ratio, 'Carlini et al.', num_augs, train_seeds),\n",
    "    ('loss', lambda x: x, get_identity, 'Yeom et al.', 1, [0]),\n",
    "    ('loss', lambda x: x, get_mast, 'Sablayrolles et al.', 1, train_seeds),\n",
    "    ('loss', lambda x: x, get_mast_offline, 'Watson et al.', 1, train_seeds),\n",
    "    ('loss', lambda x: x, get_ye_et_el_attack_r, 'Ye et al.', 1, train_seeds),\n",
    "    ('mentr', lambda x: x, get_class_based, 'Song et al.', 1, train_seeds),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 3/8 [00:03<00:06,  1.33s/it]/tmp/ipykernel_5822/2255348333.py:47: RuntimeWarning: Mean of empty slice\n",
      "  means_in.append(np.nanmean(in_scores, axis=1))\n",
      "/tmp/ipykernel_5822/2255348333.py:48: RuntimeWarning: Mean of empty slice\n",
      "  means_out.append(np.nanmean(out_scores, axis=1))\n",
      "/tmp/ipykernel_5822/2255348333.py:49: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  var_in.append(np.nanvar(in_scores, axis=1))\n",
      "/tmp/ipykernel_5822/2255348333.py:50: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  var_out.append(np.nanvar(out_scores, axis=1))\n",
      "100%|██████████| 8/8 [00:06<00:00,  1.17it/s]\n"
     ]
    }
   ],
   "source": [
    "for info_type, score_func, _, method_name, augs, seeds in tqdm(methods):\n",
    "    if method_name in scores:\n",
    "        continue\n",
    "    \n",
    "    score, mask = get_scores_and_masks(\n",
    "        seeds=seeds, \n",
    "        dataset=dataset,\n",
    "        method=info_type,\n",
    "        num_augs=augs)\n",
    "\n",
    "    score = score_func(score)\n",
    "    params = get_score_dist_params(score, mask, augs)\n",
    "    scores[method_name] = score\n",
    "    masks[method_name] = mask\n",
    "    dist_params[method_name] = params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:00<00:00, 34.02it/s]\n"
     ]
    }
   ],
   "source": [
    "test_scores = {}\n",
    "test_masks = {}\n",
    "\n",
    "for info_type, score_func, _, method_name, augs, _ in tqdm(methods):\n",
    "    test_score, test_mask = get_scores_and_masks(\n",
    "        seeds=test_seeds, \n",
    "        dataset=dataset,\n",
    "        method=info_type,\n",
    "        num_augs=augs)\n",
    "\n",
    "    test_scores[method_name] = score_func(test_score)\n",
    "    test_masks[method_name] = test_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_tpr_at_fpr(tpr, fpr, target_fpr=1e-1):\n",
    "    \"\"\"\n",
    "    Finds the TPR when FPR equals target_fpr using interpolation.\n",
    "\n",
    "    Parameters:\n",
    "    tpr (array): Array of true positive rates.\n",
    "    fpr (array): Array of false positive rates.\n",
    "    target_fpr (float): The target false positive rate. Default is 1e-1.\n",
    "\n",
    "    Returns:\n",
    "    float: Interpolated TPR at the target FPR.\n",
    "    \"\"\"\n",
    "    # Ensure the arrays are numpy arrays\n",
    "    tpr = np.array(tpr)\n",
    "    fpr = np.array(fpr)\n",
    "\n",
    "    # Use numpy's interpolation function\n",
    "    tpr_at_target_fpr = np.interp(target_fpr, fpr, tpr)\n",
    "    return tpr_at_target_fpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================== Seed 64 ======================\n",
      "AUC Curv ZO NLL: 93.55, Acc 84.62\n",
      "AUC Curv ZO LR: 90.19, Acc 80.62\n",
      "AUC Carlini et al.: 89.08, Acc 81.63\n",
      "AUC Yeom et al.: 82.14, Acc 76.37\n",
      "AUC Sablayrolles et al.: 81.32, Acc 70.41\n",
      "AUC Watson et al.: 72.22, Acc 63.04\n",
      "AUC Ye et al.: 91.03, Acc 80.82\n",
      "AUC Song et al.: 82.24, Acc 75.53\n",
      "=================== Seed 65 ======================\n",
      "AUC Curv ZO NLL: 93.66, Acc 84.62\n",
      "AUC Curv ZO LR: 90.13, Acc 80.48\n",
      "AUC Carlini et al.: 88.88, Acc 81.66\n",
      "AUC Yeom et al.: 82.48, Acc 76.73\n",
      "AUC Sablayrolles et al.: 81.44, Acc 70.59\n",
      "AUC Watson et al.: 71.74, Acc 62.79\n",
      "AUC Ye et al.: 90.99, Acc 80.97\n",
      "AUC Song et al.: 82.63, Acc 75.95\n",
      "=================== Seed 66 ======================\n",
      "AUC Curv ZO NLL: 93.23, Acc 84.17\n",
      "AUC Curv ZO LR: 90.10, Acc 80.36\n",
      "AUC Carlini et al.: 88.70, Acc 81.37\n",
      "AUC Yeom et al.: 81.72, Acc 75.79\n",
      "AUC Sablayrolles et al.: 80.55, Acc 69.65\n",
      "AUC Watson et al.: 71.01, Acc 62.30\n",
      "AUC Ye et al.: 90.62, Acc 80.40\n",
      "AUC Song et al.: 81.97, Acc 75.25\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score, roc_curve, balanced_accuracy_score\n",
    "\n",
    "auroc_all = {}\n",
    "bal_acc_all = {}\n",
    "tpr_m1 = {}\n",
    "tpr_m2 = {}\n",
    "tpr_m3 = {}\n",
    "tpr_m4 = {}\n",
    "\n",
    "for test_seed_idx, test_seed in enumerate(test_seeds):\n",
    "    print(f\"=================== Seed {test_seed} ======================\")\n",
    "    for idx, (info_type, _, pred_func, method_name, augs, _) in enumerate(methods):\n",
    "        y_pred = pred_func(\n",
    "            test_scores[method_name][..., test_seed_idx, np.newaxis],\n",
    "            dist_params[method_name],\n",
    "            augs,\n",
    "            scores[method_name],\n",
    "            masks[method_name])\n",
    "\n",
    "        y_true = test_masks[method_name][..., test_seed_idx, np.newaxis]\n",
    "        if method_name == 'Song et al.':\n",
    "            labels_file_name = os.path.join(\n",
    "                config['precomputed_scores_dir'], \n",
    "                dataset.name, \n",
    "                f\"true_labels_cifar100_{train_end_seed}.pt\")\n",
    "            labels = np.load(labels_file_name)['data']\n",
    "            auc = 0 \n",
    "            n_classes = dataset.num_classes\n",
    "            balanced_accuracy = 0\n",
    "            for label in range(n_classes):\n",
    "                indices = np.where(labels == label)[0]\n",
    "                # Compute ROC curve for each class\n",
    "                fpr, tpr, thr = roc_curve(y_true[indices], y_pred[indices])\n",
    "                auc += roc_auc_score(y_true, y_pred)\n",
    "                \n",
    "                # Find the optimal threshold: where the sum of FPR and TPR is closest to 1\n",
    "                optimal_idx = np.argmin(np.abs(fpr + tpr - 1))\n",
    "                optimal_threshold = thr[optimal_idx]\n",
    "                # Binarize predictions based on the optimal threshold\n",
    "                y_pred_binarized = (y_pred >= optimal_threshold).astype(int)\n",
    "\n",
    "                # Calculate Balanced Accuracy\n",
    "                balanced_accuracy += balanced_accuracy_score(y_true, y_pred_binarized)\n",
    "            \n",
    "            auc = auc / n_classes\n",
    "            balanced_accuracy = balanced_accuracy / n_classes\n",
    "\n",
    "        else:\n",
    "            fpr, tpr, thr = roc_curve(y_true, y_pred, pos_label=1)\n",
    "            auc = roc_auc_score(y_true, y_pred)\n",
    "\n",
    "            # Find the optimal threshold: where the sum of FPR and TPR is closest to 1\n",
    "            # \"Balanced accuracy is symmetric. That is, the metric\n",
    "            # assigns equal cost to false-positives and to false-negatives.\"\n",
    "            # - LiRA https://arxiv.org/pdf/2112.03570.pdf\n",
    "            optimal_idx = np.argmin(np.abs(fpr + tpr - 1))\n",
    "            optimal_threshold = thr[optimal_idx]\n",
    "\n",
    "            # Binarize predictions based on the optimal threshold\n",
    "            y_pred_binarized = (y_pred >= optimal_threshold).astype(int)\n",
    "\n",
    "            # Calculate Balanced Accuracy\n",
    "            balanced_accuracy = balanced_accuracy_score(y_true, y_pred_binarized)\n",
    "        \n",
    "        print(f\"AUC {method_name}: {auc*100:.2f}, Acc {balanced_accuracy * 100:.2f}\")\n",
    "        if method_name in auroc_all:\n",
    "            auroc_all[method_name] += [auc]\n",
    "            bal_acc_all[method_name] += [balanced_accuracy]\n",
    "            tpr_m1[method_name] += [find_tpr_at_fpr(tpr, fpr, target_fpr=1e-2)]\n",
    "            tpr_m2[method_name] += [find_tpr_at_fpr(tpr, fpr, target_fpr=1e-3)]\n",
    "            tpr_m3[method_name] += [find_tpr_at_fpr(tpr, fpr, target_fpr=1e-4)]\n",
    "        else:\n",
    "            auroc_all[method_name] = [auc]\n",
    "            bal_acc_all[method_name] = [balanced_accuracy]\n",
    "            tpr_m1[method_name] = [find_tpr_at_fpr(tpr, fpr, target_fpr=1e-2)]\n",
    "            tpr_m2[method_name] = [find_tpr_at_fpr(tpr, fpr, target_fpr=1e-3)]\n",
    "            tpr_m3[method_name] = [find_tpr_at_fpr(tpr, fpr, target_fpr=1e-4)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curv ZO NLL, auroc 93.48 $\\pm$ 0.18, bal 84.47 $\\pm$ 0.21\n",
      "Curv ZO LR, auroc 90.14 $\\pm$ 0.04, bal 80.48 $\\pm$ 0.11\n",
      "Carlini et al., auroc 88.89 $\\pm$ 0.16, bal 81.55 $\\pm$ 0.13\n",
      "Yeom et al., auroc 82.11 $\\pm$ 0.31, bal 76.29 $\\pm$ 0.39\n",
      "Sablayrolles et al., auroc 81.11 $\\pm$ 0.39, bal 70.22 $\\pm$ 0.41\n",
      "Watson et al., auroc 71.66 $\\pm$ 0.50, bal 62.71 $\\pm$ 0.31\n",
      "Ye et al., auroc 90.88 $\\pm$ 0.19, bal 80.73 $\\pm$ 0.24\n",
      "Song et al., auroc 82.28 $\\pm$ 0.27, bal 75.58 $\\pm$ 0.29\n"
     ]
    }
   ],
   "source": [
    "for k, v in auroc_all.items():\n",
    "    auc_for_k = np.array(v)\n",
    "    bal_acc_for_k = np.array(bal_acc_all[k])\n",
    "\n",
    "    print(f\"{k}, auroc {auc_for_k.mean() * 100:.2f} $\\pm$ {auc_for_k.std()* 100:.2f}, bal {bal_acc_for_k.mean()* 100:.2f} $\\pm$ {bal_acc_for_k.std()* 100:.2f}\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curv ZO NLL, top 1% 41.54 $\\pm$ 0.88, tpr 0.1%7.75 $\\pm$ 1.45\n",
      "Curv ZO NLL, top 0.01% 0.00 $\\pm$ 0.00\n",
      "Curv ZO LR, top 1% 35.54 $\\pm$ 0.49, tpr 0.1%23.44 $\\pm$ 1.19\n",
      "Curv ZO LR, top 0.01% 5.23 $\\pm$ 7.40\n",
      "Carlini et al., top 1% 21.78 $\\pm$ 0.64, tpr 0.1%15.80 $\\pm$ 0.49\n",
      "Carlini et al., top 0.01% 6.56 $\\pm$ 1.13\n",
      "Yeom et al., top 1% 0.94 $\\pm$ 0.06, tpr 0.1%0.07 $\\pm$ 0.01\n",
      "Yeom et al., top 0.01% 0.01 $\\pm$ 0.00\n",
      "Sablayrolles et al., top 1% 21.05 $\\pm$ 0.38, tpr 0.1%10.50 $\\pm$ 0.59\n",
      "Sablayrolles et al., top 0.01% 4.30 $\\pm$ 0.53\n",
      "Watson et al., top 1% 14.86 $\\pm$ 0.14, tpr 0.1%6.62 $\\pm$ 0.23\n",
      "Watson et al., top 0.01% 2.95 $\\pm$ 0.18\n",
      "Ye et al., top 1% 34.72 $\\pm$ 1.86, tpr 0.1%6.63 $\\pm$ 0.52\n",
      "Ye et al., top 0.01% 2.86 $\\pm$ 0.42\n",
      "Song et al., top 1% 2.15 $\\pm$ 0.74, tpr 0.1%1.21 $\\pm$ 0.37\n",
      "Song et al., top 0.01% 1.21 $\\pm$ 0.37\n"
     ]
    }
   ],
   "source": [
    "for k, v in tpr_m1.items():\n",
    "    tpr_1 = np.array(v)\n",
    "    tpr_2 = np.array(tpr_m2[k])\n",
    "    tpr_3 = np.array(tpr_m3[k])\n",
    "\n",
    "    print(f\"{k}, top 1% {tpr_1.mean() * 100:.2f} $\\pm$ {tpr_1.std()* 100:.2f}, tpr 0.1%{tpr_2.mean()* 100:.2f} $\\pm$ {tpr_2.std()* 100:.2f}\") \n",
    "    print(f\"{k}, top 0.01% {tpr_3.mean() * 100:.2f} $\\pm$ {tpr_3.std()* 100:.2f}\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
